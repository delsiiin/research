研究动机：
		联邦学习（FL）允许多个客户端在不透露数据的情况下协作训练神经网络（NN）模型。最近，一些有针对
	性的中毒攻击已经被引入。这些攻击向生成的模型注入一个后门，允许反向控制的输入被错误分类。现有的针
	对后门攻击的对策效率低效，通常仅仅旨在将偏离的模型排除在聚合之外。但是，这种方法还删除了具有偏离
	数据分布的客户机的良性模型，从而导致聚合模型对此类客户机表现不佳。

后门攻击：
		数据中毒（弱对手）--通过向数据集添加恶意攻击数据来毒害客户端的训练数据（考虑PDR；足以毒害数据集而不影响
	实际训练NN的客户端）
		模型中毒（强对手）--能够妥协客户端的一个子集并完全控制它们。然后，可以在提交模型更新之前任意地更改模型
	更新，以增加对聚合模型的攻击影响
		
传统防御方式：
		检测和删除有毒模型（与大多数策略不同的模型被认为是可疑的，不能区分不同数据分布的良性训练数据，从而导致模型性能下降）
		限制中毒攻击的影响（对攻击冲击大的中毒模型无效）

防御模型：
		过滤层：第一层使用所提出的新技术（DDifs、NEUPs、阈值超过度量）来分析模型更新，以检测和排除包含一个训练有素的后门的模型。
			特征提取（余弦、DDifs、NEUP）-标记（阈值超过度量）与聚类-中毒聚类识别（为每个聚类确定被中毒标记的模型更新的百分比）
		裁剪层：强制执行更新的最大l2范数，并在必要时降低它们，以减轻用高比例因子补偿弱训练后门（绕过第一层）的有毒模型
		聚合层：使用FedAvg将剩余的剪切更新聚合在一起

创新点：
		提出DeepSight模型过滤方法，在识别具有高攻击影响的恶意模型更新的同时保持良性模型更新
		提出了一种基于投票的模型过滤方案，结合了分类器和基于聚类的相似性估计
		提出了阈值超过度量，用于衡量训练数据的同质性
		基于三种不同的技术设计了一个聚类算法的集成
		提出两种新的技术来测量神经网络结构和输出中的细粒度差异（DDifs--划分差异、NEUPs--标准化的UPdate能量）

未来研究方向：
		需要实现结合隐私保护的DeepSight，即结合安全聚合的隐私收益的后门缓解算法。